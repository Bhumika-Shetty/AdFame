## AdFame

<!-- 
Discuss: Value proposition: Your will propose a machine learning system that can be 
used in an existing business or service. (You should not propose a system in which 
a new business or service would be developed around the machine learning system.) 
Describe the value proposition for the machine learning system. What's the (non-ML) 
status quo used in the business or service? What business metric are you going to be 
judged on? (Note that the "service" does not have to be for general users; you can 
propose a system for a science problem, for example.)
-->

### Contributors

<!-- Table of contributors and their roles. 
First row: define responsibilities that are shared by the team. 
Then, each row after that is: name of contributor, their role, and in the third column, 
you will link to their contributions. If your project involves multiple repos, you will 
link to their contributions in all repos here. -->

| Name                            | Responsible for | Link to their commits in this repo |
|---------------------------------|-----------------|------------------------------------|
| Yashdeep Prasad                 |Model serving and monitoring platforms                 |https://github.com/Bhumika-Shetty/AdFame/activity?actor=prasad-yashdeep                                    |
| Bhumika Dinesh Shetty           |Model training and training platforms               | https://github.com/Bhumika-Shetty/AdFame/activity?actor=Bhumika-Shetty                                   |
| Divij Kapur                     |Continuous X                |https://github.com/Bhumika-Shetty/AdFame/activity?actor=dk-4999                                    |
| Yashas Harisha                  |Data pipeline                 |                                    |



### System diagram

![System Architecture](./system_arch.png)

** Note- All the microservices are containerised using docker and the orchestration takes place using kubernetes.
Currently we are experimenting multiple text to Video generation models , namely - Wan2.1, Hunyuan video, Open-sora, Cogxvideo


<!-- Overall digram of system. Doesn't need polish, does need to show all the pieces. 
Must include: all the hardware, all the containers/software platforms, all the models, 
all the data. -->

### Summary of outside materials

<!-- In a table, a row for each dataset, foundation model. 
Name of data/model, conditions under which it was created (ideally with links/references), 
conditions under which it may be used. -->

|              | How it was created | Conditions of use |
|--------------|--------------------|-------------------|
| Data set 1   |                    |                   |
| Data set 2   |                    |                   |
| Base model 1 |                    |                   |
| etc          |                    |                   |


### Summary of infrastructure requirements

<!-- Itemize all your anticipated requirements: What (`m1.medium` VM, `gpu_mi100`), 
how much/when, justification. Include compute, floating IPs, persistent storage. 
The table below shows an example, it is not a recommendation. -->

| Requirement     | How many/when                                     | Justification |
|-----------------|---------------------------------------------------|---------------|
| `m1.medium` VMs | 3 for entire project duration                     | Used across three environments: "dev/staging," "canary," and "production," each operating independent CI/CD pipelines and serving models separately.         |
| `gpu_mi100`     | 4 hour block thrice a week                         |     Fine-tuning different models for text to image/video generation using 4×A100 GPUs.(Minimum VRAM 80GB or higher)          |
| Floating IPs    | 3 for entire project duration, 1 for sporadic use |     Each m1.medium environment instance is assigned one IP, with an additional IP allocated for occasional GPU usage.          |
| Persistent Storage             |             1 TB                                      |     Stores large volumes of images, videos, and model versions generated by text-to-image/video workflows.          |

### Detailed design plan

<!-- In each section, you should describe (1) your strategy, (2) the relevant parts of the 
diagram, (3) justification for your strategy, (4) relate back to lecture material, 
(5) include specific numbers. -->

#### Model training and training platforms

<!-- Make sure to clarify how you will satisfy the Unit 4 and Unit 5 requirements, 
and which optional "difficulty" points you are attempting. -->


# 🎥 Fine-Tuned Video Diffusion Model Serving

This project provides a **RESTful API** using **FastAPI** to generate videos from textual prompts via a fine-tuned **video diffusion model** (up to 14B parameters). Designed for automated marketing content creation, the service meets strict performance and reliability targets with advanced model and system-level optimizations.


## 🚀 API Endpoint

**POST** `/generate-video`  
Accepts a text prompt and returns a generated video.

### 🔧 Example Request
```json
{
  "prompt": "A sunset over the ocean in the style of a watercolor painting",
  "num_frames": 16,
  "fps": 8
}
```

### 📼 Example Response
```json
{
  "video_url": "https://example.com/videos/gen_abc123.mp4",
  "frames": 16,
  "fps": 8,
  "duration": 2.0
}
```

---

## 🎯 Performance Requirements

- **Max Model Size**: 14B parameters  
- **Batch Throughput**: 4 prompts / 2 minutes  
- **Single Prompt Latency**: < 30 seconds  
- **Concurrent Users**: 5–10 (cloud-based)  

---

## 🧠 Model-Level Optimizations

- ✅ **Mixed-Precision Inference** (FP16/BF16)
- ✅ **INT8 Static Quantization** (ONNX)
- ✅ **ONNX Graph Optimizations**: Constant folding, operator fusion
- ✅ **Optimized Attention**: FlashAttention, xFormers

### 🧩 Distributed Inference Strategies

<details>
<summary><strong>Ulysses Strategy</strong></summary>

- Use `--ulysses_size $GPU_NUMS`
- Heads must be divisible by number of GPUs
- Not suitable for 1.3B model with 12 heads on 8-GPU setup

</details>

<details>
<summary><strong>Ring Strategy</strong></summary>

- Use `--ring_size $GPU_NUMS`
- Sequence length must be divisible by ring size
- More flexible than Ulysses

</details>

<details>
<summary><strong>Hybrid Strategy</strong></summary>

- Combine Ulysses and Ring
- Example: Ulysses intra-node, Ring inter-node

</details>

### 📈 TeaCache Acceleration
- Reduces redundant computations during diffusion steps  
- Achieves up to **2× speedup**

---

## 🧰 System-Level Optimizations

- 🔄 **Asynchronous FastAPI handlers**
- 📬 **Redis-backed job queue** (manages concurrent jobs)
- 🔁 **Model warm pools** (avoids cold starts)
- ⚡ **Load balancing** with autoscaling (Kubernetes compatible)

---

## 📦 Deployment Options

| Platform           | Latency       | Throughput           | Cost             |
|-------------------|---------------|-----------------------|------------------|
| **GPU Server**     | ~20–30 sec    | 4+ videos / 2 minutes | High performance |
| **CPU (ONNX)**     | ~180+ sec      | 1–2 videos / 5 minutes| Budget-friendly  |
| **Edge Device**    | 5–10+ minutes  | 1 video / 5–10 minutes | Low cost         |



# 🛠️ Online Evaluation, Feedback Loop, and Monitoring


## 📊 Evaluation & Monitoring

### 📍 Offline Evaluation (via MLflow)

- **Text-Video Alignment**: CLIP score  
- **Realism**: Fréchet Video Distance (FVD)  
- **Temporal Consistency**: Frame similarity  
- **Fairness Testing**: Diverse demographic prompts  
- **Known Failure Modes**: Flickering, object drift  
- **Unit Tests**: Ensure API validity

### 🧪 Load Testing in Staging

- Simulates real user traffic (5–10 concurrent prompts)  
- Tests varying prompt lengths, GPU load, etc.



## 🧪 Canary Deployment Strategy

We implement a **canary deployment** approach to safely roll out new model versions:

- **Initial Split**: Deploy the new model alongside the old one. Only ~5% of real or artificial user traffic is routed to the new version.
- **Simulated Users**: We simulate a diverse range of user prompts to reflect real-world behavior. These include:
  - Short, generic prompts (e.g., "a city skyline")
  - Domain-specific prompts (e.g., branded content like "Nike shoes on a basketball court")
  - Long descriptive prompts
  - Edge-case and fairness-related prompts (e.g., diverse demographic references)

### 🔍 Monitoring During Canary Phase

- Log **generation latency**, **errors**, and **CLIP-based similarity scores**.
- Sample generated outputs for **manual inspection**.
- Compare canary metrics vs. baseline to evaluate improvements or regressions.

### 🔁 Promotion or Rollback

- If performance is stable, gradually increase canary traffic to 25%, 50%, then 100%.
- Roll back instantly to the old model if:
  - Latency spikes
  - Output quality drops
  - User complaints increase

## 🔄 Close the Feedback Loop

To continuously improve our model post-deployment:

- **User Feedback**
  - Collect ratings or regenerate signals from users
  - Log interaction metrics (e.g., time spent, downloads, shares)
- **Annotation & Ground Truth**
  - Manually review a sample of production outputs
  - Use human annotation or automated checks (e.g., brand compliance)
- **Production Data Storage**
  - Save a portion of prompt-output pairs
  - Label and use this data for periodic fine-tuning

## 📈 Business-Specific Evaluation Plan

While not fully deployed in production, we define a plan for future business-aligned evaluations:

- **Brand Compliance**: Ensure outputs align with visual identity standards (logo, colors, tone)
- **Engagement Metrics**: Monitor share/download rates as proxy for content effectiveness
- **Speed to First Frame**: Measure how quickly content can be generated and published

## 🧠 Extra Difficulty Points Attempted

### 📊 Monitor for Data & Label Drift

- Use text embeddings to monitor **prompt distribution shift**
- Detect visual drift in output content (e.g., style shifts)
- Visualize prompt clusters over time in a dashboard

### 🔧 Monitor for Model Degradation

- Track trends in CLIP scores, latency, and user behavior
- Alert engineers when thresholds are crossed
- Automatically trigger retraining with fresh labeled production data

## 🔁 Automated Recovery & Continuous Learning

We implement a **self-healing pipeline**:

1. Monitor: Log and evaluate metrics in real-time
2. Detect: Spot drift or degradation
3. Retrain: Use recent data to fine-tune model
4. Validate: Run updated model through eval pipeline
5. Deploy: Use canary rollout for safe promotion

---

> This framework ensures that the deployed video diffusion model remains accurate, fair, and aligned with user expectations—even as inputs, content demands, and business goals evolve.




#### Data pipeline

<!-- Make sure to clarify how you will satisfy the Unit 8 requirements,  and which 
optional "difficulty" points you are attempting. -->

#### Continuous X

<!-- Make sure to clarify how you will satisfy the Unit 3 requirements,  and which 
optional "difficulty" points you are attempting. -->

**Objective:** Design a cloud-native CI/CD pipeline with staged deployment, infrastructure-as-code (IaC), and automated continuous training to support the video generation and AB testing system.

---

#### **1. Infrastructure-as-Code (IaC) & Cloud-Native Design**

- **Tools:**
  - **Terraform**: Declaratively define Chameleon infrastructure (VMs, networks, storage) in Git.
  - **Ansible**: Automate software installation (Docker, Ray, MLFlow) and configuration on provisioned VMs.
  - **ArgoCD/Helm**: Manage Kubernetes deployments for microservices (LLM, video generation, resolution adjustment).
- **Immutable Infrastructure**:
  - All microservices and User Interface (frontend, LLM, video model, etc.) are containerized using Docker.
  - Updates require rebuilding containers and redeploying via Git-triggered pipelines.

---

#### **2. CI/CD Pipeline Design**

**Trigger:** Code push to `main` branch or manual trigger.  
**Stages:**

1. **Build & Test**:
   - Containerize each microservice using Docker.
   - Run unit tests for frontend, LLM prompt generation, and video model inference.
2. **Continuous Training**:
   - **Ray Cluster Integration**: Submit model retraining jobs (e.g., finetuned attention model) to Ray via Argo Workflows.
   - **Experiment Tracking**: Log metrics (e.g., AB test results) to MLFlow hosted on Chameleon.
3. **Offline Evaluation**:
   - Validate model performance on fairness, failure modes, and brand-specific scenarios.
   - Only register models in MLFlow if evaluation passes thresholds.
4. **Staging Deployment**:
   - Deploy to staging using ArgoCD. Mirror production but with fewer replicas.
5. **Load Testing**:
   - Simulate concurrent user requests (e.g., 5 parallel video inferences) using Locust.
6. **Canary Deployment**:
   - Promote to canary if staging tests pass.
7. **Production Deployment**:
   - Full rollout after canary success. Use Kubernetes autoscaling for high traffic.

---

#### **3. Staged Environments**

- **Staging**: Low-resource setup for integration testing.
- **Canary**: Partial rollout to detect regressions.
- **Production**: Scalable deployment with GPU nodes.

---

#### **4. Continuous Training Integration**

- **Triggers**:
  - Scheduled retraining (e.g., weekly).
  - User feedback from AB tests (stored in DB via Microservice 4).
- **Data Pipeline**:
  - Unit 8’s ETL processes ingest new user feedback and production data for retraining.
- **Optimizations**:
  - **Ray Train**: Distributed training for large video models (achieves "difficulty points").
  - **Ray Tune**: Hyperparameter tuning for prompt generation (LLM) and video quality.

---

#### **5. Difficulty Points Attempted**

1. **Distributed Training**: Use Ray Train to parallelize video model training across GPUs.
2. **Multiple Serving Options**: Compare GPU (high-performance) vs. CPU (cost-effective) inference for video generation.
3. **Advanced Monitoring**:
   - **Model Degradation**: Alert on AB test performance drops and trigger retraining.

---

#### **6. Validation**

- **GitHub Repo**: All IaC, Ansible playbooks, and Argo Workflows in version control.
- **Immutable Artifacts**: Docker images stored in Chameleon’s private registry.
- **Documentation**: Pipeline flowchart and failure recovery plan (e.g., rollback via Argo Rollouts).




