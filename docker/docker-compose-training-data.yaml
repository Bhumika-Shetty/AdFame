name: fashion-extraction
volumes:
  block-persist-project15:
services:
        
  extract-fashion-videos:
    container_name: fashion_extraction
    image: python:3.11
    user: root
    volumes:
      - /mnt/block:/mnt/openvid
    working_dir: /app
    command:
      - bash
      - -c
      - |
        set -e
        echo "Creating script file..."
        cat > fashion_extraction_script.py << 'EOL'
        import os
        import subprocess
        import pandas as pd
        import requests
        import shutil
        import zipfile
        from concurrent.futures import ThreadPoolExecutor, as_completed
        from tqdm import tqdm

        OUTPUT_DIRECTORY = "/mnt/openvid"
        data_folder = os.path.join(OUTPUT_DIRECTORY, "data", "train")
        os.makedirs(data_folder, exist_ok=True)

        csv_url = "https://huggingface.co/datasets/nkp37/OpenVid-1M/resolve/main/data/train/OpenVid-1M.csv"
        csv_path = os.path.join(data_folder, "OpenVid-1M.csv")
        if not os.path.exists(csv_path):
            command = ["wget", "-O", csv_path, csv_url]
            try:
                subprocess.run(command, check=True)
                print(f"Downloaded {csv_url} to {csv_path}")
            except subprocess.CalledProcessError as e:
                print(f"Failed to download {csv_url}: {e}")

        df = pd.read_csv(csv_path)
        print("CSV loaded successfully")

        male_keywords = ['male model', 'man', 'menswear', 'male fashion']
        female_keywords = ['female model', 'woman', 'womenswear', 'female fashion']
        brand_keywords = ['adidas','nike']

        def is_relevant_fashion_show(caption):
            if not isinstance(caption, str):
                return False
            caption_lower = caption.lower()
            return (
                (any(mk in caption_lower for mk in male_keywords) or
                any(fek in caption_lower for fek in female_keywords)) and
                any(bk in caption_lower for bk in brand_keywords)
            )

        filtered_df = df[df['caption'].apply(is_relevant_fashion_show)]
        filtered_df = filtered_df[[
            'video', 'caption', 'aesthetic score', 'motion score',
            'temporal consistency score', 'seconds'
        ]].rename(columns={'seconds': 'duration_seconds'})
        filtered_df = filtered_df.sort_values(by=['aesthetic score', 'motion score'], ascending=[False, False])
        print(f"Found {len(filtered_df)} relevant fashion show videos.")

        os.makedirs("/mnt/openvid/OpenVid_CSVs", exist_ok=True)
        base_url = "https://huggingface.co/datasets/phil329/OpenVid-1M-mapping/resolve/main/video_mappings/"
        for i in range(186):
            filename = f"OpenVid_part{i}.csv"
            url = base_url + filename
            csv_path = os.path.join("/mnt/openvid/OpenVid_CSVs", filename)
            if not os.path.exists(csv_path):
                r = requests.get(url)
                if r.status_code == 200:
                    with open(csv_path, "wb") as f:
                        f.write(r.content)
                    print(f"Downloaded {filename}")
                else:
                    print(f"Failed to download {filename}")
            else:
                print(f"Already exists {filename}")

        mappings = [pd.read_csv(f"/mnt/openvid/OpenVid_CSVs/OpenVid_part{i}.csv") for i in range(186)]
        full_mapping = pd.concat(mappings)
        merged_df = filtered_df.merge(full_mapping, on="video", how="left")
        print("Data merged successfully")

        needed_zips = merged_df["zip_file"].unique()
        print(f"Need to download {len(needed_zips)} zip files.")
        needed_paths = merged_df.groupby('zip_file')['video_path'].unique().to_dict()
        uuid_to_prompt = dict(zip(merged_df["video"], merged_df["caption"]))
        uuid_set = set(merged_df["video"])
        found = {}

        error_log_path = os.path.join(OUTPUT_DIRECTORY, "download_log.txt")
        zip_folder = os.path.join(OUTPUT_DIRECTORY, "zip_files")
        temp_video_folder = os.path.join(OUTPUT_DIRECTORY, "only_fashion_show_dataset")
        os.makedirs(zip_folder, exist_ok=True)
        os.makedirs(temp_video_folder, exist_ok=True)

        def download_with_progress(url, dest_path):
            try:
                response = requests.get(url, stream=True, timeout=30)
                if response.status_code != 200:
                    with open(error_log_path, "a") as log:
                        log.write(f"Invalid URL: {url} (Status code {response.status_code})\n")
                    return False
                total = int(response.headers.get('content-length', 0))
                with open(dest_path, 'wb') as f, tqdm(total=total, unit='B', unit_scale=True, desc=dest_path) as bar:
                    for chunk in response.iter_content(8192):
                        f.write(chunk)
                        bar.update(len(chunk))
                return True
            except requests.RequestException as e:
                with open(error_log_path, "a") as log:
                    log.write(f"Download error for {url}: {e}\n")
                return False

        def extract_video(zip_name):
            local_found = {}
            zip_file_path = os.path.join(zip_folder, zip_name)
            target_path = temp_video_folder

            try:
                with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
                    for path in needed_paths[zip_name]:
                        command = ['unzip', '-j', zip_file_path, path, '-d', target_path]
                        try:
                            subprocess.run(command, check=True)
                            print(f"âœ… Extracted '{path}'")
                        except subprocess.CalledProcessError as e:
                            print(f"An error occurred: {e}")
                        final_idx = len(found) + 1
                        final_video_path = os.path.join(temp_video_folder, f"a ({final_idx}).mp4")
                        final_prompt_path = os.path.join(temp_video_folder, f"a ({final_idx}).txt")
                        if os.path.exists(final_video_path):
                            os.remove(final_video_path)
                        os.rename(os.path.join(target_path, os.path.basename(path)), final_video_path)
                        with open(final_prompt_path, "w", encoding="utf-8") as f:
                            f.write(uuid_to_prompt[os.path.basename(path)])
                        found[os.path.basename(path)] = True
                return local_found
            except zipfile.BadZipFile as e:
                with open(error_log_path, "a") as log:
                    log.write(f"Bad ZIP file error ({zip_file_path}): {e}\n")
                return local_found

        def download_and_extract(zip_name):
            local_found = {}
            zip_file_path = os.path.join(zip_folder, zip_name)
            if not os.path.exists(zip_file_path):
                url = f"https://huggingface.co/datasets/nkp37/OpenVid-1M/resolve/main/{zip_name}"
                if not download_with_progress(url, zip_file_path):
                    return local_found
            local_found = extract_video(zip_name)
            os.remove(zip_file_path)
            return local_found

        with ThreadPoolExecutor(max_workers=3) as executor:
            futures = {executor.submit(download_and_extract, zip_name): zip_name for zip_name in needed_zips}  
            for future in tqdm(as_completed(futures), total=len(futures), desc="Overall Progress"):
                pass

        print(f"ðŸŽ‰ Completed. Extracted {len(found)} matching videos.")
        EOL

        echo "Installing required packages..."
        pip install pandas requests tqdm

        echo "Running fashion extraction script..."
        python fashion_extraction_script.py

  split-fashion-data:
    container_name: split_fashion_data
    image: python:3.11
    user: root
    volumes:
      - /mnt/block:/mnt/openvid
    working_dir: /app
    command:
      - bash
      - -c
      - |
        echo "Creating split script..."
        cat > split_fashion_data.py << 'EOF'
        import os
        import random
        import shutil

        temp_video_folder = "/mnt/openvid/only_fashion_show_dataset"
        print("Splitting dataset into train, validate, and production...")

        video_files = sorted([f for f in os.listdir(temp_video_folder) if f.endswith(".mp4")])
        random.seed(42)
        random.shuffle(video_files)

        n = len(video_files)
        train_cutoff = int(0.7 * n)
        validate_cutoff = int(0.85 * n)

        splits = {
            "train": video_files[:train_cutoff],
            "validate": video_files[train_cutoff:validate_cutoff],
            "production": video_files[validate_cutoff:]
        }

        for split_name, files in splits.items():
            split_dir = os.path.join(temp_video_folder, split_name)
            os.makedirs(split_dir, exist_ok=True)
            for f in files:
                txt_file = f.replace(".mp4", ".txt")
                src_video = os.path.join(temp_video_folder, f)
                src_txt = os.path.join(temp_video_folder, txt_file)
                dst_video = os.path.join(split_dir, f)
                dst_txt = os.path.join(split_dir, txt_file)
                if os.path.exists(src_video):
                    shutil.move(src_video, dst_video)
                if os.path.exists(src_txt):
                    shutil.move(src_txt, dst_txt)

        print("âœ… Dataset successfully split.")
        EOF

        echo "Running split script..."
        python split_fashion_data.py

  load-data:
    container_name: etl_load_data
    image: rclone/rclone:latest
    volumes:
      - /mnt/block:/mnt/openvid
      - ~/.config/rclone/rclone.conf:/root/.config/rclone/rclone.conf:ro
    entrypoint: /bin/sh
    command:
      - -c
      - |
        if [ -z "$RCLONE_CONTAINER" ]; then
          echo "ERROR: RCLONE_CONTAINER is not set"
          exit 1
        fi
        echo "Cleaning up existing contents of container..."
        rclone delete chi_tacc:$RCLONE_CONTAINER --rmdirs || true

        rclone copy /mnt/openvid chi_tacc:$RCLONE_CONTAINER \
        --progress \
        --transfers=32 \
        --checkers=16 \
        --multi-thread-streams=4 \
        --fast-list

        echo "Listing directories in container after load stage:"
        rclone lsd chi_tacc:$RCLONE_CONTAINER

