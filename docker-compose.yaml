version: '3.8'

services:
  training_app:
    build:
      context: . # Assumes Dockerfile is in /home/cc/bhumi/AdFame/
      dockerfile: Dockerfile
    container_name: training_app_container
    # Changed command to keep container running without executing script
    command: ["tail", "-f", "/dev/null"]
    # Changed working_dir to the correct location of schedule_ray.py
    working_dir: /app/AdFame/trainig_pipeline/diffusion-pipe 
    environment:
      - MLFLOW_TRACKING_URI=http://mlflow_server:5000 
      - RAY_ADDRESS=ray://ray_head:10001
      - NVIDIA_VISIBLE_DEVICES=all
      - NCCL_P2P_DISABLE=1
      - NCCL_IB_DISABLE=1
    volumes:
      - ./trainig_pipeline:/app/AdFame/trainig_pipeline
      - /home/cc/bhumi/AdFame/trainig_pipeline/diffusion-pipe/mlruns:/app/AdFame/trainig_pipeline/diffusion-pipe/mlruns
      - /mnt/data/train:/app/AdFame/trainig_pipeline/diffusion-pipe/data/input
      - /home/cc/bhumi/AdFame/trainig_pipeline/diffusion-pipe/data/output:/app/AdFame/trainig_pipeline/diffusion-pipe/data/output
      # - /home/cc/bhumi/AdFame/trainig_pipeline/diffusion-pipe/models/Wan2.1-T2V-14B:/home/cc/bhumi/AdFame/trainig_pipeline/diffusion-pipe/models/Wan2.1-T2V-14B
    depends_on:
      - mlflow_server
      - ray_head
    shm_size: '16gb'
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    networks:
      - app_network

  mlflow_server:
    image: ghcr.io/mlflow/mlflow:latest
    container_name: mlflow_server_container
    restart: always
    ports:
      - "5000:5000"
    volumes:
      - /home/cc/bhumi/AdFame/trainig_pipeline/diffusion-pipe/mlruns:/app/AdFame/trainig_pipeline/diffusion-pipe/mlruns
    command: >
      mlflow server
      --host 0.0.0.0
      --port 5000
      --backend-store-uri sqlite:////mlflow_host_data/mlflow.db 
      --default-artifact-root file:///mlflow_host_data
    networks:
      - app_network

  ray_head:
    image: rayproject/ray-ml:latest-py38-gpu
    container_name: ray_head_container
    restart: always
    ports:
      - "10001:10001"
      - "8265:8265"
      - "6379:6379"
    environment:
      - RAY_HEAD_SERVICE_HOST=ray_head
      - NVIDIA_VISIBLE_DEVICES=all
    command: /bin/bash -c "chmod -R 777 /tmp/ray && ray start --head --port=6379 --ray-client-server-port=10001 --dashboard-host=0.0.0.0 --num-cpus=4 --num-gpus=1"
    shm_size: '8gb'
    volumes:
      - ray_head_logs:/tmp/ray
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    networks:
      - app_network
    user: root

  prometheus_server:
    image: prom/prometheus:latest
    container_name: prometheus_server_container
    restart: always
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    command: --config.file=/etc/prometheus/prometheus.yml --storage.tsdb.path=/prometheus
    networks:
      - app_network

  grafana_server:
    image: grafana/grafana:latest
    container_name: grafana_server_container
    restart: always
    ports:
      - "3000:3000"
    volumes:
      - grafana_data:/var/lib/grafana
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_DATASOURCES_DEFAULT_NAME=Prometheus
      - GF_DATASOURCES_DEFAULT_TYPE=prometheus
      - GF_DATASOURCES_DEFAULT_URL=http://prometheus_server:9090
      - GF_DATASOURCES_DEFAULT_ACCESS=proxy
      - GF_DATASOURCES_DEFAULT_IS_DEFAULT=true
    depends_on:
      - prometheus_server
    networks:
      - app_network

volumes:
  prometheus_data:
  grafana_data:
  ray_head_logs:

networks:
  app_network:
    driver: bridge
